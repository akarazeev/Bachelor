\chapter{Существующие алгоритмы} \label{ch:ch2}

\section{Анализ} \label{sec:ch2/sec1}

В основном в качестве источника информации используются статьи \cite{dai, hodge, vakili, varun, billor, wilkinson}. Помимо статей интерес представляют наборы данных, которые тоже предстояло найти. Среди наборов данных есть хорошо известный MNIST с набором изображений рукописных цифр, а также данные о разных сортах винных изделий, свободных электронах в ионосфере Земли, о заболеваниях сердца и другие.

В этой работе рассматриваются стационарные данные. Поиск аномалий во временных рядах, а также прогнозирование временных рядов находятся за рамками рассматриваемой в работе темы.

\section{Основы алгоритмов} \label{sec:ch2/sec2}

Задачу поиска аномалий можно отнести к классу задач обучения без учителя. Суть поиска аномалий заключается в том, чтобы найти в выборке объекты, которые не похожи на большинство объектов выборки, т. е. те, которые выделяются на фоне других.

Часто бывает так, что аномальных объектов либо нет вообще, либо их очень мало и неизвестно где именно в выборке они находятся. Поэтому поиск аномалий относится к классу задач обучения без учителя (т. к. отсутствуют размеченные данные).

\section{Результат работы алгоритмов} \label{sec:ch1/sec2}

Результатом работы алгоритма для поиска аномалий могут быть как \textbf{степени аномалии} (anomaly scores), так и \textbf{бинарные метки} (binary labels).

В случае, когда алгоритм выдаёт \textbf{степень аномалии}, под степенью понимается уровень вероятности того, что объект является аномалией.

В случае \textbf{бинарных меток} алгоритм сразу указывает на нормальные (обычно обозначаемые как 0) и аномальные (обозначаемые как 1) данные. Несмотря на то, что некоторые алгоритмы детектирования аномалий возвращают бинарные метки напрямую, степени аномалий тоже могут быть переведены в бинарное представление. 0 или 1 содержат меньше информации, чем степень аномалии. Тем не менее, это конечный результат, по которому обычно принимается решение об аномальности объекта выборки.

\section{Классификация методов определения аномалий} \label{sec:ch1/sec3}

Большинство методов определения аномалий используют метки, по которым можно определить, является ли объект выборки нормальным или аномальным. Поиск или сбор размеченных данных, которые будут точными и хорошо описывать рассматриваемую проблемы, чтобы хорошо обучить алгоритмы, довольно сложно и дорого.

\noindent Обычно выделяют три типа методов поиска аномалий:

\begin{enumerate}
	\item \textbf{Supervised методы (обучение с учителем)}

Предполагается, что имеется доступ к обучающим данным с точными и репрезентативными метками для нормальных и аномальных объектов. В таком случае обычно разрабатывают предсказательную модель для обоих классов. После обучения на тренировочных данных к каждому объекту из тестовой выборки применяется алгоритм, чтобы определить класс объекта. 
\todo{Однако есть и проблема: получить точные и репрезентативные метки, особенно для аномалий, сложно. Поскольку аномалия определяется через смесь нескольких атрибутов. Такая ситуация довольно распространена в сценариях, таких как обнаружение мошенников в потоке информации в банковском секторе (сложно отличить мошенника от обычного пользователя только по действиям).}
	
	\item \textbf{Semi-supervised методы (обучение с частичным привлечение учителя)}

Предполагается, что имеются размеченные данные только для нормального класса. Так как для обучения таких алгоритмов не требуются размеченные аномальные данные, они имеют более широкое применение, чем supervised методы.
	\item \textbf{Unsupervised методы (обучение без учителя)}

Такие методы не требуют обучающих данных и поэтому наиболее широко используются. Unsupervised методы поиска аномалий могут нормальные данные из всех представленных и рассматривать отклонение от них как аномалию.
\end{enumerate}
Многие semi-supervised методы могут быть использованы для unsupervised случая. Например, с их помощью можно дополнительно семплировать объекты из выборки, если данных для обучения алгоритма попросту недостаточно.

\subsection{Метрика качества модели}

В качестве основной метрики используется ROC-AUC, что расшифровывается как receiver operating characteristic -- area under curve и переводится как "рабочая характеристика приёмника -- площадь под кривой". ROC-кривая позволяет оценить качество бинарной классификации\footnote{бинарная означает, что есть лишь два класса объектов -- например, нормальные и аномальные}. Она показывает соотношение между долей объектов, которые были верно классифицированы как принадлежащие определённому классу (True Positive Rate, TPR), от общего числа объектов в выборке Именно площадь под кривой и выступает в роли метрики качества. Минимальное значение, которое может принимать ROC-AUC составляет 0.5, а максимальное -- 1. 

\todo{Дальше будет описано чем это вызвано.}

\todo{ROC-кривая (англ. receiver operating characteristic, рабочая характеристика приёмника) — график, позволяющий оценить качество бинарной классификации, отображает соотношение между долей объектов от общего количества носителей признака, верно классифицированных как несущих признак, (англ. true positive rate, TPR, называемой чувствительностью алгоритма классификации) и долей объектов от общего количества объектов, не несущих признака, ошибочно классифицированных как несущих признак (англ. false positive rate, FPR, величина 1-FPR называется специфичностью алгоритма классификации) при варьировании порога решающего правила. Также известна как кривая ошибок. Анализ классификаций с применением ROC-кривых называется ROC-анализом. Количественную интерпретацию ROC даёт показатель AUC (англ. area under ROC curve, площадь под ROC-кривой) — площадь, ограниченная ROC-кривой и осью доли ложных положительных классификаций. Чем выше показатель AUC, тем качественнее классификатор, при этом значение 0,5 демонстрирует непригодность выбранного метода классификации (соответствует случайному гаданию). Значение менее 0,5 говорит, что классификатор действует с точностью до наоборот: если положительные назвать отрицательными и наоборот, классификатор будет работать лучше.}

\begin{figure}[ht]
  \centering
  \includegraphics [scale=0.7] {roc_curve}
  \caption{Пример ROC-кривой.}
  \label{fig:roc_curve}
\end{figure}

График на рисунке \ref{fig:roc_curve} был построен с помощью библиотеки scikit-learn\footnote{\url{https://scikit-learn.org/0.15/auto_examples/plot_roc.html}}.

\clearpage

\section{Подходы к решению} \label{sec:ch2/sec3}

Одним из возможных способов определения аномалий является измерение схожести между объектов. У такого способа есть два варианта:
\begin{enumerate}
  \item Восстановление плотности
  \item Классификация
\end{enumerate}

\subsection{Восстановление плотности}

В случае с восстановлением плотности необходимо построить распределение, которое хорошо описывает выборку. И это распределение позволяет посчитать вероятность для нового объекта получить его из распределения, описывающего выборку.

В терминах этого метода аномалия - объект, полученный из другого распределения, описывающего другую выборку данных.

\noindent Есть три подхода:
\begin{enumerate}
  \item Параметрический
  \item Непараметрический
  \item Восстановление смесей
\end{enumerate}

\noindent\textbf{Параметрический метод} -- Распределение представляется в виде $p(x)=\phi(x\vert\theta)$, где $\theta$ выступает в качестве параметра распределения. Например, в семейство параметрических распределений входит распределение Гаусса - $\theta=(\mu, \Sigma)$, где $\mu$ - вектор средних и $\Sigma$ - ковариационная матрица.

Параметры модели подбираются таким образом, чтобы вероятность объектов из обучающей выборки была максимальной. Для этого обычно пользуются Методом Максимального Правдоподобия:

\[ \sum_{i}\log\phi(x_{i}\vert\theta) \rightarrow \max_{\theta} \]

\todo{Есть ещё Автокодировщики, которые хорошо справляются с понижением размерности данных и с помощью которых можно хорошо семплировать из заданного распределения \cite{karazeev}.}

\clearpage

\section{Алгоритмы} \label{sec:ch2/sec4}

\noindent В текущей работе были рассмотрены следующие алгоритмы:

\begin{enumerate}
	\item k-Nearest Neighbors (k-NN) \cite{knn} -- метод k-ближайших соседей.
	\item Principal Component Analysis (PCA) \cite{pca} -- метод главных компонент.
	\item One-Class Support Vector Machines (OCSVM) \cite{ocsvm} -- одноклассовый метод опорных векторов.
	\item Local Outlier Factor (LOF) \cite{lof} -- метод локального уровеня выброса.
	\item Histogram-Based Outlier Score (HBOS) \cite{hbos} -- оценка выбросов на основе гистограммы.
	\item Isolation Forest \cite{iforest} -- метод изолируещего леса.
\end{enumerate}

\subsection{k-Nearest Neighbors (k-NN)}

\todo{Метрический алгоритм для автоматической классификации объектов. Объект присваивается тому классу, который является наиболее распространённым среди k соседей данного элемента, классы которых уже известны. Алгоритм может быть применим к выборкам с большим количеством атрибутов (многомерным). Для этого перед применением нужно определить функцию расстояния; классический вариант такой функции -- евклидова метрика.} На~рисунке~\ref{fig:knn} приведён пример классификации k-ближайших соседей.

\begin{figure}[ht]
  \centering
  \includegraphics [scale=0.1] {knn}
  \caption{\todo{Тестовый образец (зелёный круг) должен быть классифицирован как синий квадрат (класс 1) или как красный треугольник (класс 2). Если $k = 3$, то он классифицируется как 2-й класс, потому что внутри меньшего круга 2 треугольника и только 1 квадрат. Если $k = 5$, то он будет классифицирован как 1-й класс (3 квадрата против 2 треугольников внутри большего круга)}}
  \label{fig:knn}
\end{figure}

\subsection{Principal Component Analysis (PCA)}

\todo{Алгоритм для уменьшения размерности данных, потеряв наименьшее количество информации. Применяется во многих областях, в том числе, в эконометрике, биоинформатике, обработке изображений, для сжатия данных, в общественных науках. Вычисление главных компонент может быть сведено к вычислению сингулярного разложения матрицы данных или к вычислению собственных векторов и собственных значений ковариационной матрицы исходных данных.}

\begin{figure}[ht]
  \centering
  \includegraphics [scale=0.1] {pca}
  \caption{\todo{PCA для многомерного гауссового распределения с центром в точке (1, 3) со стандартным отклонением 3. Векторы показывают отражают собственные векторы ковариационной матрицы}}
  \label{fig:pca}
\end{figure}

\subsection{One-Class Support Vector Machines (OCSVM)}

\todo{Алгоритм принадлежит семейству линейных классификаторов. Особым свойством метода опорных векторов является непрерывное уменьшение эмпирической ошибки классификации и увеличение зазора, поэтому метод также известен как метод классификатора с максимальным зазором. Основная идея метода — перевод исходных векторов в пространство более высокой размерности и поиск разделяющей гиперплоскости с максимальным зазором в этом пространстве. Две параллельных гиперплоскости строятся по обеим сторонам гиперплоскости, разделяющей классы. Разделяющей гиперплоскостью будет гиперплоскость, максимизирующая расстояние до двух параллельных гиперплоскостей. Алгоритм работает в предположении, что чем больше разница или расстояние между этими параллельными гиперплоскостями, тем меньше будет средняя ошибка классификатора.}

\begin{figure}[ht]
  \centering
  \includegraphics [scale=0.1] {svm}
  \caption{\todo{H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximal margin.}}
  \label{fig:svm}
\end{figure}

\subsection{Local Outlier Factor (LOF)}

\todo{Локальный уровень выброса основывается на концепции локальной плотности, где локальность задаётся k ближайшими соседями, расстояния до которых используются для оценки плотности. Путём сравнения локальной плотности объекта с локальной плотностью его соседей, можно выделить области с аналогичной плотностью и точки, которые имеют существенно меньшую плотность, чем её соседи. Эти точки считаются выбросами. Локальная плотность оценивается типичным расстоянием, с которым точка может быть «достигнута» от соседних точек. Определение «расстояния достижимости», используемого в алгоритме, является дополнительной мерой для получения более устойчивых результатов внутри кластеров.}

\begin{figure}[ht]
  \centering
  \includegraphics [scale=0.1] {lof}
  \caption{\todo{Базовая идея метода -- сравнение локальной плотности точки с плотностями её соседей. Точка A имеет меньшую плотность по сравнению с соседями}}
  \label{fig:lof}
\end{figure}

\subsection{Histogram-Based Outlier Score (HBOS)}

Алгоритм HBOS в несколько раз быстрее работает, чем алгоритмы, основанные на кластеризации и методе ближайших соседей. Для каждого измерения $d$ строится одномерная гистограмма, где высота каждой ячейки отражает оценку плотности. Затем проводится нормировка таким образом, что максимальная высота ячеек каждой гистограммы составляет $1.0$. Это обеспечивает равный вклад каждого измерения в оценку аномальности. Наконец, для каждого объекта $p$ выборки рассчитывается $HBOS$, используя высоту соответствующей ячейки, в которой объект расположен: \[HBOS\left(p\right) = \sum_{i=0}^{d}\log\left(\frac{1}{hist_i\left(p\right)}\right)\] Вместо произведения используется сумма логарифмов -- это является тем же самым, что и применение логарифма к произведению $\left(\log\left(a\cdot b\right) = \log\left(a\right)+\log\left(b\right)\right)$. Такой подход менее чувствительный к ошибкам, которые связаны с точностью плавающей точки в экстремально несбалансированных распределениях, что в свою очередь может приводить к очень высоким значениям оценки аномальности.

\subsection{Isolation Forest}

\todo{Один из вариантов случайного леса (строится из решающих деревьев). Выбирается случайный признак и случайное расщепление, по которым строится ветвление в дереве. Для каждого объекта выборки определяется мера его нормальности как среднее арифметическое глубин листьев, в которые он попал (под этим понимается "изоляция"). При таком способе построения деревьев аномалии будут попадать в листья на ранних этапах (на небольшой глубине дерева), т.е. выбросы проще «изолировать». Дерево строится до тех пор, пока каждый объект не окажется в отдельном листе).}

\begin{figure}[ht]
  \centering
  \includegraphics [scale=0.7] {iforest}
  \caption{Для изолирования точки $x_i$ требуется 12 случайных разбиений, а для аномальной точки $x_o$ -- только 4 разбиения.}
  \label{fig:iforest}
\end{figure}

\clearpage

\section{Наборы данных} \label{sec:ch2/sec5}

\noindent Для проверки сервиса были рассмотрены следующие наборы данных:
\begin{enumerate}
  \item Arrhythmia -- определение наличия аритмии по данным ЭКГ \cite{guvenir}.
  \item Breast Cancer -- определение типа опухоли молочной железы: доброкачественная или злокачественная.
  \item Glass -- идентификация типа стекла, оставленного на месте преступления.
  \item Ionosphere -- рассматриваются характеристики радаров, которые используется в анализе ионосферы: необходимо определить является радар "плохим" или "хорошим".
  \item Letter Recognition -- по описанию изображения определить присутствует ли буква из английского алфавита или нет.
  \item Mammography -- детектирование микрокальцинатов по данным маммографии.
  \item MNIST -- научиться различать изображения рукописных цифр 6 и 0.
  \item Satellite -- определение типа почвы по спутниковым снимкам.
\end{enumerate}
В качестве источника этих данных выступает библиотека ODDS \cite{odds}.

\begin{table} [htbp]
	\centering
	\caption{Статистика по данным из рассматриваемых наборов данных.}\label{tab:stats}%
	\begin{tabular}{lrrr}
		\toprule
		     Датасет & Кол-во объектов & Размерность &  Процент аномалий \\
		\midrule
		  arrhythmia &      452 &         274 &      14.60 \\
		     breastw &      683 &           9 &      34.99 \\
		       glass &      214 &           9 &       4.21 \\
		  ionosphere &      351 &          33 &      35.90 \\
		      letter &     1600 &          32 &       6.25 \\
		 mammography &    11183 &           6 &       2.33 \\
		       mnist &     7603 &         100 &       9.21 \\
		   satellite &     6435 &          36 &      31.64 \\
		\bottomrule
		\hline
	\end{tabular}
\end{table}

Сравнение данных, которые представлены в указанных наборах данных.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth, height=\textheight, keepaspectratio] {2d_comparison}
  \caption{Рассматриваемые наборы данных после применения алгоритма понижения размерности t-SNE.}
  \label{fig:2d_comparison}
\end{figure}

\begin{table} [htbp]
	\centering
	\caption{Значения ROC для рассматриваемых алгоритмов на данных.}\label{tab:rocs}%
	\begin{tabular}{lrrrrrr}
		\toprule
		     Датасет &   KNN &   PCA &  OCSVM &   LOF &  HBOS &  IFOREST \\
		\midrule   
   		arrhythmia &  0.7555 &  0.7794 &  0.7825 &  0.7672 &  0.7831 &   \textbf{0.7849} \\
     breastw &  \textbf{0.9908} &  0.9608 &  0.9649 &  0.4574 &  0.9764 &   0.9872 \\
       glass &  \textbf{0.8558} &  0.7308 &  0.8077 &  0.6538 &  0.7500 &   0.7212 \\
  ionosphere &  \textbf{0.9460} &  0.8115 &  0.8684 &  0.9023 &  0.6190 &   0.8632 \\
      letter &  \textbf{0.8660} &  0.5119 &  0.5985 &  0.8530 &  0.5532 &   0.5770 \\
 mammography &  0.8346 &  \textbf{0.9039} &  0.8911 &  0.6806 &  0.8506 &   0.8680 \\
       mnist &  0.8322 &  \textbf{0.8493} &  0.8487 &  0.6727 &  0.5607 &   0.7942 \\
   satellite &  0.6795 &  0.5601 &  0.6274 &  0.5567 &  \textbf{0.7464} &   0.7008 \\
		\bottomrule
		\hline
	\end{tabular}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth, height=\textheight, keepaspectratio] {roc_vs_time}
  \caption{Эффективность алгоритмов на разных наборах данных.}
  \label{fig:roc_vs_time}
\end{figure}

При построении графиков, которые изображены на рисунке~\ref{fig:roc_vs_time}, использовалась библиотека adjustText\footnote{\url{https://github.com/Phlya/adjustText/}}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth, height=\textheight, keepaspectratio] {roc_vs_dataset}
  \caption{Качество алгоритмов в зависимости от набора данных.}
  \label{fig:roc_vs_dataset}
\end{figure}

Демонстрация наилучших результатов работы алгоритмов (рисунок~\ref{fig:d_breastw}).

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth, height=\textheight, keepaspectratio] {d_breastw}
  \caption{Датасет Breast Cancer.}
  \label{fig:d_breastw}
\end{figure}

\clearpage

%\begin{figure}[ht]
%  \centering
%  \includegraphics [scale=0.27] {latex}
%  \caption{TeX.}
%  \label{fig:latex}
%\end{figure}
